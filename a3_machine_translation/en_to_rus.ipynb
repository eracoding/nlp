{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "683a7b10-c777-415e-9653-9f34f0667906",
   "metadata": {},
   "source": [
    "# **Machine Translation with Transformers from Scratch**\n",
    "\n",
    "## **Project Overview**\n",
    "This project focuses on building a machine translation system from **English to Russian** using **transformers implemented from scratch**. The primary goal is to experiment with and compare different **attention mechanism variations** to evaluate their impact on translation quality.\n",
    "\n",
    "## **Objectives**\n",
    "- Implement a **transformer-based** translation model from scratch.\n",
    "- Explore and compare **various attention mechanisms** (e.g., scaled dot-product attention, local attention, and adaptive attention).\n",
    "- Train the model on an **English-Russian parallel dataset**.\n",
    "- Evaluate translation quality using **Perplexity metric**.\n",
    "- Optimize performance by fine-tuning architectural components.\n",
    "\n",
    "## **Key Components**\n",
    "- **Data Preprocessing:** Tokenization, text normalization, and preparation of parallel English-Russian datasets.\n",
    "- **Model Architecture:** Implementation of the transformer model, including encoder-decoder structures and attention mechanisms.\n",
    "- **Training & Optimization:** Training the model with effective hyperparameters and loss functions.\n",
    "- **Evaluation:** Measuring translation accuracy using BLEU scores and analyzing model performance.\n",
    "\n",
    "## **Expected Outcomes**\n",
    "- A working machine translation model that translates **English to Russian** with high accuracy.\n",
    "- Insights into how **different attention mechanisms** affect translation quality.\n",
    "- Potential improvements in efficiency and translation fluency by fine-tuning model components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50ff96b5-06df-4108-89d2-f81bd0251ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import datasets\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import random, math, time, copy, gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23f21844-b3b3-45b2-9955-0f20ffe59d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global vars\n",
    "# randomness\n",
    "SEED = 42\n",
    "\n",
    "# data processing\n",
    "SRC_LANGUAGE = 'en'\n",
    "TRG_LANGUAGE = 'ru'\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "\n",
    "# model_related\n",
    "BATCH_SIZE = 8\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e38d3987-b72d-4ed7-95c3-c9504b5f0120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0+cu121 0.16.2+cpu\n"
     ]
    }
   ],
   "source": [
    "# Pre-configs\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "# device2 = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# lib versions\n",
    "print(torch.__version__, torchtext.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f075ee5-ae58-4d0c-a166-6f5d876212f1",
   "metadata": {},
   "source": [
    "## ETL: Loading the dataset\n",
    "\n",
    "The dataset chosen is random phrases in the following [link](https://huggingface.co/datasets/Helsinki-NLP/opus-100/viewer/en-ru)\n",
    "\n",
    "OPUS-100 is an English-centric multilingual corpus covering 100 languages.\n",
    "\n",
    "OPUS-100 is English-centric, meaning that all training pairs include English on either the source or target side. The corpus covers 100 languages (including English). The languages were selected based on the volume of parallel data available in OPUS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3517c2fe-8d85-4308-88f3-7ce70c6a0a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# books = datasets.load_dataset(\"opus_books\", \"en-ru\") # not good, want more data\n",
    "dataset = datasets.load_dataset('opus100', 'en-ru')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaf6ed6-9093-4811-9c01-8bf8b96e7970",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_size = 50000 # selecting 50k of training samples since it is getting a lot of time to train\n",
    "dataset['train'] = dataset['train'].select(range(subset_size))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf1707b-5ea7-4c6b-ac15-d16c7ac8d62b",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "For preprocessing part, I want to split translation into corresponding languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fb955f-c2d7-459c-a291-3c8d4e2b4de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = dataset.map(lambda text: {lang: text['translation'][lang] for lang in ('en', 'ru')}, remove_columns=['translation'])\n",
    "preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0549a459-6cd9-4c04-8e95-34e83eb8d39b",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "Note: the models must first be downloaded using the following on the command line:\n",
    "```\n",
    "python3 -m spacy download en_core_web_sm\n",
    "python3 -m spacy download ru_core_news_sm\n",
    "```\n",
    "First, since we have two languages, let's create some constants to represent that. Also, let's create two dicts: one for holding our tokenizers and one for holding all the vocabs with assigned numbers for each unique word.\n",
    "\n",
    "#### Text to integers (Numericalization)\n",
    "Next we gonna create function (torchtext called vocabs) that turn these tokens into integers. Here we use built in factory function ```build_vocab_from_iterator``` which accepts iterator that yield list or iterator of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce12a63f-05fb-44c9-a7a6-421ccb9ba998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "token_transform[TRG_LANGUAGE] = get_tokenizer('spacy', language='ru_core_news_sm')\n",
    "\n",
    "print(preprocessed['train'][23])\n",
    "print(token_transform[SRC_LANGUAGE](preprocessed['train'][SRC_LANGUAGE][23]))\n",
    "print(token_transform[TRG_LANGUAGE](preprocessed['train'][TRG_LANGUAGE][23]))\n",
    "\n",
    "def yield_tokens(data, language):\n",
    "    language_index = {SRC_LANGUAGE: 0, TRG_LANGUAGE: 1}\n",
    "    \n",
    "    for data_sample in data:\n",
    "        yield token_transform[language](data_sample[language])\n",
    "\n",
    "\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(preprocessed['train'], ln), \n",
    "                                                    min_freq=2,   #if not, everything will be treated as UNK\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True) #indicates whether to insert symbols at the beginning or at the end                                            \n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)\n",
    "\n",
    "print(vocab_transform[SRC_LANGUAGE](['hy', 'my', 'name', 'is', 'Ulugbek']))\n",
    "print(vocab_transform[TRG_LANGUAGE](['Здравствуйте', 'меня', 'зовут', 'Улугбек']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d737286-2518-4475-a0c9-ffb2dbfb7711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving vocabulary for future usage\n",
    "torch.save(vocab_transform, 'mt_enru_vocab_opus100.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75350c9c-995f-43a4-9bc5-a8f89cc8d8b3",
   "metadata": {},
   "source": [
    "## Preparing the dataloader\n",
    "We defined special symbols <unk>, <pad>, <sos>, <eos> with indexes 0, 1, 2, 3 respectively. Where each symbol has meanings as such:\n",
    "```\n",
    "<unk>: unknown\n",
    "<pad>: padding\n",
    "<sos>: Start of Sentence\n",
    "<eos>: End of Sentence\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d1c361-3e88-4c1e-badd-f639d8b9e6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = [(data[SRC_LANGUAGE], data[TRG_LANGUAGE]) for data in preprocessed['train']]\n",
    "# test  = [(data[SRC_LANGUAGE], data[TRG_LANGUAGE]) for data in preprocessed['test']]\n",
    "# val   = [(data[SRC_LANGUAGE], data[TRG_LANGUAGE]) for data in preprocessed['validation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cefc65-6882-4408-a47e-00ec2c84e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = vocab_transform[SRC_LANGUAGE].get_itos()\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for src_sample, trg_sample in batch:\n",
    "        processed_text = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
    "        src_batch.append(processed_text)\n",
    "        # print(src_sample)\n",
    "        # print(processed_text)\n",
    "        trg_batch.append(text_transform[TRG_LANGUAGE](trg_sample.rstrip(\"\\n\")))\n",
    "        src_len_batch.append(processed_text.size(0))\n",
    "    \n",
    "    # print(src_batch)\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first = True) #<----need this because we use linear layers mostly\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first = True)\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59880b3d-2feb-45a6-a577-4336c497fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "# valid_loader = DataLoader(val,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "# test_loader  = DataLoader(test,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc77c99d-377a-4255-a592-de438d9e0dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for en, _, ru in train_loader:\n",
    "#     print(en.shape)\n",
    "#     print(ru.shape)\n",
    "#     # print(en)\n",
    "#     # print(ru)\n",
    "#     break\n",
    "\n",
    "# torch.Size([8, 52])\n",
    "# torch.Size([8, 47])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4cdc0f-2c2f-4ebe-9c26-dd00e05c2483",
   "metadata": {},
   "source": [
    "### Model architecture\n",
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284a148a-d243-4c3b-8fbe-f5ec0a908b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, atten_type, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, atten_type, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, 1, 1, src len]   #if the token is padding, it will be 1, otherwise 0\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        src     = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "\n",
    "        _src    = self.feedforward(src)\n",
    "        src     = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "\n",
    "        return src\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, atten_type, device, max_length = 552):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.atten_type = atten_type\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, atten_type,device)\n",
    "                                           for _ in range(n_layers)])\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(self.device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len    = src.shape[1]\n",
    "        \n",
    "        pos        = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, src_len]\n",
    "        \n",
    "        src        = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        return src\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c41066-b075-4e5a-8b3c-ebc3de4034e0",
   "metadata": {},
   "source": [
    "### Attention layers\n",
    "\n",
    "Multihead: $$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7aabab-68fa-49d9-8e11-3fa237d91ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, atten_type, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim  = hid_dim\n",
    "        self.n_heads  = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        self.atten_type = atten_type\n",
    "\n",
    "        assert hid_dim % n_heads == 0, \"hid_dim must be divisible by n_heads\"\n",
    "        \n",
    "        self.fc_q     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale    = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "\n",
    "        if atten_type == 'additive':\n",
    "            self.W_q = nn.Linear(self.head_dim, self.head_dim)\n",
    "            self.W_k = nn.Linear(self.head_dim, self.head_dim)\n",
    "            self.v = nn.Linear(self.head_dim, 1)\n",
    "                \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        #src, src, src, src_mask\n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        #Q=K=V: [batch_size, src len, hid_dim]\n",
    "        \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        #Q = [batch_size, n heads, query len, head_dim]\n",
    "\n",
    "        # Calculate attention scores based on the selected attention variant\n",
    "        if self.atten_type == 'general':\n",
    "            energy = torch.matmul(Q, K.permute(0, 1, 3, 2))\n",
    "        elif self.atten_type == \"multiplicative\":\n",
    "            energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        elif self.atten_type == \"additive\":\n",
    "            Q_exp  = Q.unsqueeze(3)\n",
    "            K_exp  = K.unsqueeze(2)\n",
    "            energy = self.v(torch.tanh(self.W_q(Q_exp) + self.W_k(K_exp))).squeeze(-1)            \n",
    "        else:\n",
    "            raise Exception(\"Choose between 'multiplicative', 'general', or 'additive'\")\n",
    "        \n",
    "        #Q = [batch_size, n heads, query len, head_dim] @ K = [batch_size, n heads, head_dim, key len]\n",
    "        #energy = [batch_size, n heads, query len, key len]\n",
    "        \n",
    "        #for making attention to padding to 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "            \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        #attention = [batch_size, n heads, query len, key len]\n",
    "        \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        #[batch_size, n heads, query len, key len] @ [batch_size, n heads, value len, head_dim]\n",
    "        #x = [batch_size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  #we can perform .view\n",
    "        #x = [batch_size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        return x, attention\n",
    "\n",
    "\n",
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = [batch size, src len, hid dim]\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8918d9c-68b2-410f-aafb-5451d20eb08b",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23680cb-df74-46cf-a3cf-b1fc3320ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, atten_type, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm  = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, atten_type, device)\n",
    "        self.encoder_attention    = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, atten_type, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg     = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg             = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        #attention = [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        _trg = self.feedforward(trg)\n",
    "        trg  = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        return trg, attention\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
    "                 pf_dim, dropout, atten_type, device,max_length = 552):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([DecoderLayer(hid_dim, n_heads, pf_dim, dropout, atten_type, device)\n",
    "                                            for _ in range(n_layers)])\n",
    "        self.fc_out        = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len    = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, trg len]\n",
    "        \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            \n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        #attention: [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        #output = [batch_size, trg len, output_dim]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decaeb5d-046c-46b5-a473-49378622d6ce",
   "metadata": {},
   "source": [
    "### Putting them together (become Seq2Seq!)\n",
    "\n",
    "Our `trg_sub_mask` will look something like this (for a target with 5 tokens):\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "The \"subsequent\" mask is then logically anded with the padding mask, this combines the two masks ensuring both the subsequent tokens and the padding tokens cannot be attended to. For example if the last two tokens were `<pad>` tokens the mask would look like:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "\\end{matrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd7650e-b484-4246-920a-33c5e70a5ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d13a6cc-14f1-4d6b-a497-631e21deb049",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf88d026-dc8e-49e1-94f1-fcbc6d6760e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def initialize_weights(m):\n",
    "#     if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "#         nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0c823d-a6b9-42d2-b1f0-520f4e9ef423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_DIM = len(vocab_transform[SRC_LANGUAGE])\n",
    "# OUTPUT_DIM = len(vocab_transform[TRG_LANGUAGE])\n",
    "\n",
    "# ATTEN_TYPE = 'additive'\n",
    "\n",
    "# enc = Encoder(INPUT_DIM, \n",
    "#               HID_DIM, \n",
    "#               ENC_LAYERS, \n",
    "#               ENC_HEADS, \n",
    "#               ENC_PF_DIM, \n",
    "#               ENC_DROPOUT, \n",
    "#               ATTEN_TYPE,\n",
    "#               device)\n",
    "\n",
    "# dec = Decoder(OUTPUT_DIM, \n",
    "#               HID_DIM, \n",
    "#               DEC_LAYERS, \n",
    "#               DEC_HEADS, \n",
    "#               DEC_PF_DIM, \n",
    "#               DEC_DROPOUT, \n",
    "#               ATTEN_TYPE,\n",
    "#               device)\n",
    "\n",
    "# SRC_PAD_IDX = PAD_IDX\n",
    "# TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "# model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "# model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf671964-d699-47ea-8716-58fefb102554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_parameters(model):\n",
    "#     params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "#     for item in params:\n",
    "#         print(f'{item:>6}')\n",
    "#     print(f'______\\n{sum(params):>6}')\n",
    "\n",
    "# count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e50bc66-15a3-4bcd-9ef5-b6984afc1e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 0.0005\n",
    "\n",
    "# #training hyperparameters\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cd7465-2513-463b-8536-1bac7bca07d6",
   "metadata": {},
   "source": [
    "Then, we'll define our training loop. This is the exact same as the one used in the previous tutorial.\n",
    "\n",
    "As we want our model to predict the `<eos>` token but not have it be an input into our model we simply slice the `<eos>` token off the end of the sequence. Thus:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\n",
    "\\text{trg[:-1]} &= [sos, x_1, x_2, x_3]\n",
    "\\end{align*}$$\n",
    "\n",
    "$x_i$ denotes actual target sequence element. We then feed this into the model to get a predicted sequence that should hopefully predict the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "$y_i$ denotes predicted target sequence element. We then calculate our loss using the original `trg` tensor with the `<sos>` token sliced off the front, leaving the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\\\\\n",
    "\\text{trg[1:]} &= [x_1, x_2, x_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "We then calculate our losses and update our parameters as is standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61514f73-0789-43fb-ae98-5508a929d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_len, trg in loader:\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #trg[:, :-1] remove the eos, e.g., \"<sos> I love sushi\" since teaching forcing, the input does not need to have eos\n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg    = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.reshape(-1, output_dim)\n",
    "        trg = trg[:,1:].reshape(-1) #trg[:, 1:] remove the sos, e.g., \"i love sushi <eos>\" since in teaching forcing, the output does not have sos\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg    = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length\n",
    "\n",
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_len, trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0eb961-1fbb-40e0-9985-cb92f2754548",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "Finally, we train our actual model. This model is almost 3x faster than the convolutional sequence-to-sequence model and also achieves a lower validation perplexity!\n",
    "\n",
    "**Note: similar to CNN, this model always has a teacher forcing ratio of 1, i.e. it will always use the ground truth next token from the target sequence (this is simply because CNN do everything in parallel so we cannot have the next token). This means we cannot compare perplexity values against the previous models when they are using a teacher forcing ratio that is not 1. To understand this, try run previous tutorials with teaching forcing ratio of 1, you will get very low perplexity.  **   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e883a8c-2472-40bb-b68f-f41d4a5a06b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader_length = len(list(iter(train_loader)))\n",
    "# val_loader_length   = len(list(iter(valid_loader)))\n",
    "# test_loader_length  = len(list(iter(test_loader)))\n",
    "\n",
    "# # train_loader_length, val_loader_length, test_loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e9dce94-7a96-44dc-a1a1-2e5aedbed0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader_length = len(list(iter(train_loader)))\n",
    "# val_loader_length   = len(list(iter(valid_loader)))\n",
    "# test_loader_length  = len(list(iter(test_loader)))\n",
    "\n",
    "# train_max_seq = max([max(batch[0].shape[1], batch[2].shape[1]) for batch in list(iter(train_loader))])\n",
    "# val_max_seq = max([max(batch[0].shape[1], batch[2].shape[1]) for batch in list(iter(valid_loader))])\n",
    "# test_max_seq = max([max(batch[0].shape[1], batch[2].shape[1]) for batch in list(iter(test_loader))])\n",
    "\n",
    "# train_max_seq, val_max_seq, test_max_seq # required for model matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4f64ad5-e2a9-4b2a-a633-d157eba39743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def epoch_time(start_time, end_time):\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     elapsed_mins = int(elapsed_time / 60)\n",
    "#     elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "#     return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a46276b-b2ab-4155-af39-49b4351d3a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_valid_loss = float('inf')\n",
    "# num_epochs = 15\n",
    "# clip       = 1\n",
    "\n",
    "# save_path = f'models/{time.asctime()}_{model.__class__.__name__}.pt'\n",
    "\n",
    "# train_losses = []\n",
    "# valid_losses = []\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "    \n",
    "#     start_time = time.time()\n",
    "\n",
    "#     train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "#     print(\"TRAIN ONE ITER TIME: \", str(time.time() - start_time))\n",
    "#     valid_loss = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "    \n",
    "#     #for plotting\n",
    "#     train_losses.append(train_loss)\n",
    "#     valid_losses.append(valid_loss)\n",
    "    \n",
    "#     end_time = time.time()\n",
    "    \n",
    "#     epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    \n",
    "#     if valid_loss < best_valid_loss:\n",
    "#         best_valid_loss = valid_loss\n",
    "#         torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "#     print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "#     print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "#     print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "#     #lower perplexity is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce913b9-3f71-4512-815e-7e5fb9c75efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [(data[SRC_LANGUAGE], data[TRG_LANGUAGE]) for data in preprocessed['train']]\n",
    "test_data  = [(data[SRC_LANGUAGE], data[TRG_LANGUAGE]) for data in preprocessed['test']]\n",
    "val_data   = [(data[SRC_LANGUAGE], data[TRG_LANGUAGE]) for data in preprocessed['validation']]\n",
    "\n",
    "INPUT_DIM = len(vocab_transform[SRC_LANGUAGE])\n",
    "OUTPUT_DIM = len(vocab_transform[TRG_LANGUAGE])\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "N_EPOCHS = 5\n",
    "\n",
    "clip = 1\n",
    "lr = 0.0005\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    # for item in params:\n",
    "        # print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "\n",
    "# count_parameters(model)\n",
    "\n",
    "def plot_train(train_loss, valid_loss, attention):\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    \n",
    "    ax.plot(train_loss, label='Training Loss', color='blue')\n",
    "    ax.plot(valid_loss, label='Validation Loss', color='orange')\n",
    "    \n",
    "    plt.title(f'{attention}: Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(f'plots/traingraph_{attention}_{time.asctime()}.png')\n",
    "    # plt.show()\n",
    "\n",
    "def display_attention(sentence, translation, attention, fname='multihead_attention'):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=10)\n",
    "    \n",
    "    y_ticks =  [''] + translation\n",
    "    x_ticks =  [''] + sentence \n",
    "     \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.savefig(f'plots/attention_{fname}_{time.asctime()}.png')\n",
    "\n",
    "    # plt.show()\n",
    "    # plt.close()\n",
    "\n",
    "\n",
    "for attention in ['general', 'multiplicative']:# ['additive']:#:\n",
    "    if attention == 'additive':\n",
    "        BATCH_SIZE = 2\n",
    "    else:\n",
    "        BATCH_SIZE = 16\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "    valid_loader = DataLoader(val_data,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "    test_loader  = DataLoader(test_data,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "    train_loader_length = len(list(iter(train_loader)))\n",
    "    val_loader_length   = len(list(iter(valid_loader)))\n",
    "    test_loader_length  = len(list(iter(test_loader)))\n",
    "    \n",
    "    train_max_seq = max([max(batch[0].shape[1], batch[2].shape[1]) for batch in list(iter(train_loader))])\n",
    "    val_max_seq = max([max(batch[0].shape[1], batch[2].shape[1]) for batch in list(iter(valid_loader))])\n",
    "    test_max_seq = max([max(batch[0].shape[1], batch[2].shape[1]) for batch in list(iter(test_loader))])\n",
    "\n",
    "    max_seq_len = max(train_max_seq, val_max_seq, test_max_seq)\n",
    "\n",
    "    enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              attention,\n",
    "              device,\n",
    "              max_length=max_seq_len)\n",
    "\n",
    "    dec = Decoder(OUTPUT_DIM, \n",
    "                  HID_DIM, \n",
    "                  DEC_LAYERS, \n",
    "                  DEC_HEADS, \n",
    "                  DEC_PF_DIM, \n",
    "                  DEC_DROPOUT, \n",
    "                  attention,\n",
    "                  device,\n",
    "                  max_length=max_seq_len)\n",
    "    \n",
    "\n",
    "    model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "    model.apply(initialize_weights)\n",
    "\n",
    "    #training hyperparameters\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) #combine softmax with cross entropy\n",
    "\n",
    "    save_path = f'models/{attention}_{time.asctime()}_{model.__class__.__name__}.pt'\n",
    "\n",
    "    best_train_loss = float('inf')\n",
    "    best_valid_loss = float('inf')\n",
    "    best_model = None\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    print(f'\\n\\t\\t\\t {attention}')\n",
    "\n",
    "    start_training_time = time.time()\n",
    "    avg_epoch_time = 0\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        print(\"============= \", epoch)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "        valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "\n",
    "        #for plotting\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        end_time = time.time()\n",
    "        avg_epoch_time += end_time - start_time\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "\n",
    "        if valid_loss <= best_valid_loss:\n",
    "            best_train_loss = train_loss\n",
    "            best_valid_loss = valid_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "        #lower perplexity is better\n",
    "\n",
    "    plot_train(train_losses, valid_losses, attention)\n",
    "\n",
    "    best_train_ppl = math.exp(best_train_loss)\n",
    "    best_valid_ppl = math.exp(best_valid_loss)\n",
    "    \n",
    "    # Calculate time taken for the traning\n",
    "    avg_time = avg_epoch_time / N_EPOCHS\n",
    "    overall_time = epoch_time(start_training_time, end_time)\n",
    "    \n",
    "    print(f\"Best Training Loss: {best_train_loss:.3f}\")\n",
    "    print(f\"Best Validation Loss: {best_valid_loss:.3f}\")\n",
    "    print(f\"Best Training PPL: {best_train_ppl:.3f}\")\n",
    "    print(f\"Best Validation PPL: {best_valid_ppl:.3f}\")\n",
    "    print(f\"Average Time per epoch: {avg_time}\")\n",
    "    print(f\"Overall time taken: {overall_time[0]}m {overall_time[1]}s\")\n",
    "\n",
    "    test_loss = evaluate(best_model, test_loader, criterion, test_loader_length)\n",
    "\n",
    "    print(f'\\n| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')\n",
    "\n",
    "    src_txt = text_transform[SRC_LANGUAGE](preprocessed['test'][23]['en']).to(device)\n",
    "    trg_txt = text_transform[TRG_LANGUAGE](preprocessed['test'][23]['ru']).to(device)\n",
    "    src_txt = src_txt.reshape(1, -1)\n",
    "    trg_txt = trg_txt.reshape(1, -1)\n",
    "    text_length = torch.tensor([src_txt.size(0)]).to(dtype=torch.int64)\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        output, attention_plot = best_model(src_txt, trg_txt) #turn off teacher forcing\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    inference_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Inference time: {inference_time}s\")\n",
    "\n",
    "    output = output.squeeze(0)\n",
    "    output = output[1:]\n",
    "    output_max = output.argmax(1) #returns max indices\n",
    "\n",
    "    attention_plot = attention_plot[0, 0, :, :]\n",
    "    src_tokens = ['<sos>'] + token_transform[SRC_LANGUAGE](preprocessed['test'][23]['en']) + ['<eos>']\n",
    "    trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max]\n",
    "\n",
    "    display_attention(src_tokens, trg_tokens, attention_plot, fname=attention)\n",
    "    \n",
    "    # clearing cache\n",
    "    torch.cuda.empty_cache()\n",
    "    # freeing memory\n",
    "    del enc\n",
    "    del dec\n",
    "    del model\n",
    "    del best_model\n",
    "\n",
    "# Additive\n",
    "# Best Training Loss: 6.728\n",
    "# Best Validation Loss: 6.441\n",
    "# Best Training PPL: 835.796\n",
    "# Best Validation PPL: 626.890\n",
    "# Average Time per epoch: 1214.0751595973968\n",
    "# Overall time taken: 101m 10s\n",
    "\n",
    "# | Test Loss: 6.435 | Test PPL: 623.397 |\n",
    "# Inference time: 0.010747194290161133s\n",
    "\n",
    "# General\n",
    "\n",
    "# \t\t\t general\n",
    "# =============  0\n",
    "# Epoch: 01 | Time: 2m 34s\n",
    "# \tTrain Loss: 6.444 | Train PPL: 628.898\n",
    "# \t Val. Loss: 5.997 |  Val. PPL: 402.082\n",
    "# =============  1\n",
    "# Epoch: 02 | Time: 2m 34s\n",
    "# \tTrain Loss: 6.059 | Train PPL: 427.850\n",
    "# \t Val. Loss: 6.003 |  Val. PPL: 404.625\n",
    "# =============  2\n",
    "# Epoch: 03 | Time: 2m 33s\n",
    "# \tTrain Loss: 6.086 | Train PPL: 439.830\n",
    "# \t Val. Loss: 6.324 |  Val. PPL: 557.725\n",
    "# =============  3\n",
    "# Epoch: 04 | Time: 2m 32s\n",
    "# \tTrain Loss: 6.257 | Train PPL: 521.510\n",
    "# \t Val. Loss: 6.545 |  Val. PPL: 695.625\n",
    "# =============  4\n",
    "# Epoch: 05 | Time: 2m 29s\n",
    "# \tTrain Loss: 6.052 | Train PPL: 425.169\n",
    "# \t Val. Loss: 6.141 |  Val. PPL: 464.631\n",
    "# Best Training Loss: 6.444\n",
    "# Best Validation Loss: 5.997\n",
    "# Best Training PPL: 628.898\n",
    "# Best Validation PPL: 402.082\n",
    "# Average Time per epoch: 152.9643747806549\n",
    "# Overall time taken: 12m 44s\n",
    "\n",
    "# | Test Loss: 5.977 | Test PPL: 394.320 |\n",
    "# Inference time: 0.011830329895019531s\n",
    "\n",
    "# \t\t\t multiplicative\n",
    "# =============  0\n",
    "# Epoch: 01 | Time: 2m 54s\n",
    "# \tTrain Loss: 5.969 | Train PPL: 391.149\n",
    "# \t Val. Loss: 5.469 |  Val. PPL: 237.140\n",
    "# =============  1\n",
    "# Epoch: 02 | Time: 2m 35s\n",
    "# \tTrain Loss: 5.282 | Train PPL: 196.823\n",
    "# \t Val. Loss: 5.118 |  Val. PPL: 166.928\n",
    "# =============  2\n",
    "# Epoch: 03 | Time: 2m 53s\n",
    "# \tTrain Loss: 4.934 | Train PPL: 138.982\n",
    "# \t Val. Loss: 4.939 |  Val. PPL: 139.599\n",
    "# =============  3\n",
    "# Epoch: 04 | Time: 2m 54s\n",
    "# \tTrain Loss: 4.672 | Train PPL: 106.889\n",
    "# \t Val. Loss: 4.880 |  Val. PPL: 131.669\n",
    "# =============  4\n",
    "# Epoch: 05 | Time: 2m 45s\n",
    "# \tTrain Loss: 4.465 | Train PPL:  86.908\n",
    "# \t Val. Loss: 4.834 |  Val. PPL: 125.679\n",
    "# Best Training Loss: 4.465\n",
    "# Best Validation Loss: 4.834\n",
    "# Best Training PPL: 86.908\n",
    "# Best Validation PPL: 125.679\n",
    "# Average Time per epoch: 168.6279025554657\n",
    "# Overall time taken: 14m 8s\n",
    "\n",
    "# | Test Loss: 4.808 | Test PPL: 122.545 |\n",
    "# Inference time: 0.0119171142578125s\n",
    "\n",
    "# Multiplicative is performing best for 5 epochs, lets train it longer up to 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b9a66d-f003-44db-ad24-34f3003c6b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INPUT_DIM = len(vocab_transform[SRC_LANGUAGE])\n",
    "OUTPUT_DIM = len(vocab_transform[TRG_LANGUAGE])\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "N_EPOCHS = 5\n",
    "\n",
    "clip = 1\n",
    "lr = 0.0005\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    # for item in params:\n",
    "        # print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "\n",
    "# count_parameters(model)\n",
    "\n",
    "def plot_train(train_loss, valid_loss, attention):\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    \n",
    "    ax.plot(train_loss, label='Training Loss', color='blue')\n",
    "    ax.plot(valid_loss, label='Validation Loss', color='orange')\n",
    "    \n",
    "    plt.title(f'{attention}: Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(f'plots/traingraph_{attention}_{time.asctime()}.png')\n",
    "    # plt.show()\n",
    "\n",
    "def display_attention(sentence, translation, attention, fname='multihead_attention'):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=10)\n",
    "    \n",
    "    y_ticks =  [''] + translation\n",
    "    x_ticks =  [''] + sentence \n",
    "     \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.savefig(f'plots/attention_{fname}_{time.asctime()}.png')\n",
    "\n",
    "    # plt.show()\n",
    "    # plt.close()\n",
    "    \n",
    "# lets train multiplicative further up to 30 epochs\n",
    "for attention in ['multiplicative']:# ['additive']:#:'general', \n",
    "    if attention == 'additive':\n",
    "        BATCH_SIZE = 2\n",
    "    else:\n",
    "        BATCH_SIZE = 16\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "    valid_loader = DataLoader(val_data,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "    test_loader  = DataLoader(test_data,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "    train_loader_length = len(list(iter(train_loader)))\n",
    "    val_loader_length   = len(list(iter(valid_loader)))\n",
    "    test_loader_length  = len(list(iter(test_loader)))\n",
    "    \n",
    "    train_max_seq = max([max(batch[0].shape[1], batch[2].shape[1]) for batch in list(iter(train_loader))])\n",
    "    val_max_seq = max([max(batch[0].shape[1], batch[2].shape[1]) for batch in list(iter(valid_loader))])\n",
    "    test_max_seq = max([max(batch[0].shape[1], batch[2].shape[1]) for batch in list(iter(test_loader))])\n",
    "\n",
    "    max_seq_len = max(train_max_seq, val_max_seq, test_max_seq)\n",
    "\n",
    "    enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              attention,\n",
    "              device,\n",
    "              max_length=max_seq_len)\n",
    "\n",
    "    dec = Decoder(OUTPUT_DIM, \n",
    "                  HID_DIM, \n",
    "                  DEC_LAYERS, \n",
    "                  DEC_HEADS, \n",
    "                  DEC_PF_DIM, \n",
    "                  DEC_DROPOUT, \n",
    "                  attention,\n",
    "                  device,\n",
    "                  max_length=max_seq_len)\n",
    "    \n",
    "\n",
    "    model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "    model.apply(initialize_weights)\n",
    "\n",
    "    #training hyperparameters\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) #combine softmax with cross entropy\n",
    "\n",
    "    save_path = f'models/{attention}_{time.asctime()}_{model.__class__.__name__}.pt'\n",
    "\n",
    "    best_train_loss = float('inf')\n",
    "    best_valid_loss = float('inf')\n",
    "    best_model = None\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    print(f'\\n\\t\\t\\t {attention}')\n",
    "\n",
    "    start_training_time = time.time()\n",
    "    avg_epoch_time = 0\n",
    "\n",
    "    for epoch in range(30):\n",
    "\n",
    "        print(\"============= \", epoch)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "        valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "\n",
    "        #for plotting\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        end_time = time.time()\n",
    "        avg_epoch_time += end_time - start_time\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "\n",
    "        if valid_loss <= best_valid_loss:\n",
    "            best_train_loss = train_loss\n",
    "            best_valid_loss = valid_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "        #lower perplexity is better\n",
    "\n",
    "    plot_train(train_losses, valid_losses, attention)\n",
    "\n",
    "    best_train_ppl = math.exp(best_train_loss)\n",
    "    best_valid_ppl = math.exp(best_valid_loss)\n",
    "    \n",
    "    # Calculate time taken for the traning\n",
    "    avg_time = avg_epoch_time / 30\n",
    "    overall_time = epoch_time(start_training_time, end_time)\n",
    "    \n",
    "    print(f\"Best Training Loss: {best_train_loss:.3f}\")\n",
    "    print(f\"Best Validation Loss: {best_valid_loss:.3f}\")\n",
    "    print(f\"Best Training PPL: {best_train_ppl:.3f}\")\n",
    "    print(f\"Best Validation PPL: {best_valid_ppl:.3f}\")\n",
    "    print(f\"Average Time per epoch: {avg_time}\")\n",
    "    print(f\"Overall time taken: {overall_time[0]}m {overall_time[1]}s\")\n",
    "\n",
    "    test_loss = evaluate(best_model, test_loader, criterion, test_loader_length)\n",
    "\n",
    "    print(f'\\n| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')\n",
    "\n",
    "    src_txt = text_transform[SRC_LANGUAGE](preprocessed['test'][23]['en']).to(device)\n",
    "    trg_txt = text_transform[TRG_LANGUAGE](preprocessed['test'][23]['ru']).to(device)\n",
    "    src_txt = src_txt.reshape(1, -1)\n",
    "    trg_txt = trg_txt.reshape(1, -1)\n",
    "    text_length = torch.tensor([src_txt.size(0)]).to(dtype=torch.int64)\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        output, attention_plot = best_model(src_txt, trg_txt) #turn off teacher forcing\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    inference_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Inference time: {inference_time}s\")\n",
    "\n",
    "    output = output.squeeze(0)\n",
    "    output = output[1:]\n",
    "    output_max = output.argmax(1) #returns max indices\n",
    "\n",
    "    attention_plot = attention_plot[0, 0, :, :]\n",
    "    src_tokens = ['<sos>'] + token_transform[SRC_LANGUAGE](preprocessed['test'][23]['en']) + ['<eos>']\n",
    "    trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max]\n",
    "\n",
    "    display_attention(src_tokens, trg_tokens, attention_plot, fname=attention)\n",
    "    \n",
    "    # clearing cache\n",
    "    torch.cuda.empty_cache()\n",
    "    # freeing memory\n",
    "    del enc\n",
    "    del dec\n",
    "    del model\n",
    "    del best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dec5ca2-007d-4d63-87eb-3b4ca87a0c4b",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "It can be seen that the model is quite overfitting since validation set is not going down. Anyways, we are selecting best model based on validation set perplexity. Nonetheless, the training is taking a lot of time, will proceed for analysis based on current obtained results.\n",
    "\n",
    "| Attentions | Training Loss | Traning PPL | Validation Loss | Validation PPL | Test Loss | Test PPL | AVG time per epoch | Overall time taken |\n",
    "|----------|----------|----------|----------|----------|-|-|-|-|\n",
    "| General Attention    | 6.444     | 628.898     | 5.997     | 402.082     | 5.977 | 394.320 | 152.9s | 12m 44s |\n",
    "| Multiplicative Attention    | 4.465     | 86.908     | 4.834     | 125.679     | 4.808 | 122.545 | 168.6s | 14m 8s |\n",
    "| Additive Attention    | 6.728     | 835.796     | 6.441     | 626.890     | 4.677 | 107.489 |1214s | 101m 10s |\n",
    "\n",
    "```\n",
    "The analysis of the results demonstrates that Multiplicative Attention outperforms the other mechanisms across all metrics, achieving the lowest training loss (4.465), training perplexity (86.908), validation loss (4.834), validation perplexity (125.679), test loss (4.808), and test perplexity (122.545). This suggests that Multiplicative Attention learns patterns effectively and generalizes well to unseen data, despite slightly higher computational cost with an average epoch time of 168.6 seconds and overall training time of 14 minutes 8 seconds. General Attention performs moderately well, with higher loss and perplexity across training, validation, and test datasets, but is faster, with an average epoch time of 152.9 seconds and total training time of 12 minutes 44 seconds, making it a viable option when computational efficiency is critical. Additive Attention, however, underperforms significantly, exhibiting the highest losses and perplexities across all datasets (e.g., training loss of 6.728 and perplexity of 835.796), indicating poor learning and generalization. The results suggest that Multiplicative Attention is the optimal choice for tasks requiring high accuracy, while General Attention may be considered for resource-constrained scenarios, and Additive Attention may require further tuning to improve its performance.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78ecbcf9-5ed1-49ce-857b-9e717e60a8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 4.677 | Test PPL: 107.489 |\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(vocab_transform[SRC_LANGUAGE])\n",
    "OUTPUT_DIM = len(vocab_transform[TRG_LANGUAGE])\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "N_EPOCHS = 5\n",
    "\n",
    "train_data = [(data[SRC_LANGUAGE], data[TRG_LANGUAGE]) for data in preprocessed['train']]\n",
    "test_data  = [(data[SRC_LANGUAGE], data[TRG_LANGUAGE]) for data in preprocessed['test']]\n",
    "val_data   = [(data[SRC_LANGUAGE], data[TRG_LANGUAGE]) for data in preprocessed['validation']]\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(val_data,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test_data,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "train_loader_length = len(list(iter(train_loader)))\n",
    "val_loader_length   = len(list(iter(valid_loader)))\n",
    "test_loader_length  = len(list(iter(test_loader)))\n",
    "\n",
    "train_max_seq = max([max(batch[0].shape[1], batch[2].shape[1]) for batch in list(iter(train_loader))])\n",
    "val_max_seq = max([max(batch[0].shape[1], batch[2].shape[1]) for batch in list(iter(valid_loader))])\n",
    "test_max_seq = max([max(batch[0].shape[1], batch[2].shape[1]) for batch in list(iter(test_loader))])\n",
    "\n",
    "max_seq_len = max(train_max_seq, val_max_seq, test_max_seq)\n",
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "          HID_DIM, \n",
    "          ENC_LAYERS, \n",
    "          ENC_HEADS, \n",
    "          ENC_PF_DIM, \n",
    "          ENC_DROPOUT, \n",
    "          'multiplicative',\n",
    "          device,\n",
    "          max_length=max_seq_len)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              'multiplicative',\n",
    "              device,\n",
    "              max_length=max_seq_len)\n",
    "\n",
    "\n",
    "model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "\n",
    "save_path = 'models/multiplicative_Sun Feb  2 09:26:59 2025_Seq2SeqTransformer.pt'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) #combine softmax with cross entropy\n",
    "\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "test_loss = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')\n",
    "#| Test Loss: 4.677 | Test PPL: 107.489 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c77f4b8",
   "metadata": {},
   "source": [
    "### Evaluation of Attention Mechanisms\n",
    "\n",
    "The analysis of the results demonstrates that **Multiplicative Attention** outperforms the other mechanisms across all metrics, achieving the lowest training loss (4.465), training perplexity (86.908), validation loss (4.834), validation perplexity (125.679), test loss (4.808), and test perplexity (122.545). This suggests that Multiplicative Attention learns patterns effectively and generalizes well to unseen data, despite slightly higher computational cost with an average epoch time of 168.6 seconds and overall training time of 14 minutes 8 seconds.\n",
    "\n",
    "**General Attention** performs moderately well, with higher loss and perplexity across training, validation, and test datasets. It is faster, with an average epoch time of 152.9 seconds and total training time of 12 minutes 44 seconds, making it a viable option when computational efficiency is critical. However, its generalization to unseen data is less effective compared to Multiplicative Attention.\n",
    "\n",
    "**Additive Attention** underperforms significantly, exhibiting the highest losses and perplexities across all datasets (e.g., training loss of 6.728 and perplexity of 835.796). Despite having the best test perplexity (107.489) among the three mechanisms, its extremely high training and validation losses indicate overfitting and poor learning during training. Additionally, its computational cost is significantly higher, with an average epoch time of 1214 seconds and overall training time of 101 minutes 10 seconds.\n",
    "\n",
    "### Conclusion\n",
    "The results suggest that **Multiplicative Attention** is the optimal choice for tasks requiring high accuracy and generalization, albeit with a slightly higher computational cost. **General Attention** may be considered for resource-constrained scenarios where training time is a priority over accuracy. **Additive Attention** requires further tuning or architectural improvements to enhance its performance and computational efficiency.\n",
    "\n",
    "Inference time for all models is good - 0.01-0.05 seconds on average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc717f6-1c96-4df8-83cb-d7151ef4eeeb",
   "metadata": {},
   "source": [
    "# Final result\n",
    "\n",
    "Between three attentions, I am gonna choose multiplicative (attention used in transformers). I will deploy the dash application with this model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
