{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b770901-df3c-4fa5-9284-4ec3bdc7f528",
   "metadata": {},
   "source": [
    "# BERT - Bidirectional Encoder Representations from Transformers\n",
    "GPU enabled\n",
    "\n",
    "BERT is designed to pretrain deep bidirectional representations from\n",
    "unlabeled text by jointly conditioning on both\n",
    "left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer\n",
    "to create state-of-the-art models for a wide\n",
    "range of tasks, such as question answering and\n",
    "language inference, without substantial taskspecific architecture modifications.\n",
    "\n",
    "BERT is conceptually simple and empirically\n",
    "powerful. It obtains new state-of-the-art results on eleven natural language processing\n",
    "tasks, including pushing the GLUE score to\n",
    "80.5% (7.7% point absolute improvement),\n",
    "MultiNLI accuracy to 86.7% (4.6% absolute\n",
    "improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\n",
    "(5.1 point absolute improvement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0407fb77-8c23-48bf-a0c5-919dc43bbbec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Built-in imports\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "# from random import *\n",
    "\n",
    "# Basic processing and visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Huggingface dataset loader\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aa3f5c-a693-4ec1-b5db-109dfc0098a4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets choose available gpus for our multigpu training\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"0, 1, 3\"\n",
    "\n",
    "# Puffer proxy - redundant\n",
    "# os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "# GPU selection\n",
    "# choosing last gpu as main since others are occupied\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_gpus = torch.cuda.device_count()\n",
    "# torch.cuda.device_count(), device\n",
    "\n",
    "# Randomness configs\n",
    "SEED = 52\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Global contstants\n",
    "BATCH_SIZE = 32\n",
    "MAX_MASK   = 5 # max masked tokens when 15% exceed, it will only be max_pred\n",
    "MAX_LEN    = 1000 # maximum of length to be padded; <- reduce if gpu memory issue\n",
    "num_epoch = 100 # CAPTTALIZE?\n",
    "\n",
    "# Model params\n",
    "n_layers = 12    # number of Encoder of Encoder Layer\n",
    "n_heads  = 12    # number of heads in Multi-Head Attention\n",
    "d_model  = 768  # Embedding Size\n",
    "d_ff = d_model * 4  # 4*d_model, FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_segments = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6d3bc2-4826-4588-8433-4262bd99c562",
   "metadata": {},
   "source": [
    "## 1. Data\n",
    "\n",
    "For simplicity, we shall use very simple data like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535ea9e1-3433-4e45-84b0-4040b1c66d29",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset('bookcorpus', split='train[:1%]')\n",
    "# Dataset({\n",
    "#     features: ['text'],\n",
    "#     num_rows: 740042\n",
    "# })\n",
    "\n",
    "sentences = dataset['text'][:10000] # will take only 10k sentences since the training time is too slow\n",
    "text = [x.lower() for x in sentences]\n",
    "text = [re.sub(\"[.,!?\\\\-]\", '', x) for x in text]\n",
    "# len(text)\n",
    "#740042\n",
    "\n",
    "# # Combine everything into one to make vocab\n",
    "word_list = list(set(\" \".join(text).split()))\n",
    "word2id = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}  # special tokens\n",
    "\n",
    "# Create the word2id in a single pass\n",
    "for i, w in tqdm(enumerate(word_list), desc=\"Creating word2id\"):\n",
    "    word2id[w] = i + 4  # because 0-3 are already occupied\n",
    "\n",
    "# Precompute the id2word mapping (this can be done once after word2id is fully populated)\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "vocab_size = len(word2id)\n",
    "# 60305 (unique words)\n",
    "\n",
    "# List of all tokens for the whole text\n",
    "token_list = []\n",
    "\n",
    "# Process sentences more efficiently\n",
    "for sentence in tqdm(text, desc=\"Processing sentences\"):\n",
    "    token_list.append([word2id[word] for word in sentence.split()]) # \"Hello darkness my old friend\".split() -> numericalization i.e. [3, 5, 2, 1, 6]\n",
    "\n",
    "#token_list.size == text.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b6ad3c-adeb-47b9-abcb-c896516b3600",
   "metadata": {},
   "source": [
    "## 2. Data loader\n",
    "\n",
    "We gonna make dataloader.  Inside here, we need to make two types of embeddings: **token embedding** and **segment embedding**\n",
    "\n",
    "1. **Token embedding** - Given “The cat is walking. The dog is barking”, we add [CLS] and [SEP] >> “[CLS] the cat is walking [SEP] the dog is barking”. \n",
    "\n",
    "2. **Segment embedding**\n",
    "A segment embedding separates two sentences, i.e., [0 0 0 0 1 1 1 1 ]\n",
    "\n",
    "3. **Masking**\n",
    "As mentioned in the original paper, BERT randomly assigns masks to 15% of the sequence. In this 15%, 80% is replaced with masks, while 10% is replaced with random tokens, and the rest 10% is left as is.  Here we specified `max_pred` \n",
    "\n",
    "4. **Padding**\n",
    "Once we mask, we will add padding. For simplicity, here we padded until some specified `max_len`. \n",
    "\n",
    "Note:  `positive` and `negative` are just simply counts to keep track of the batch size.  `positive` refers to two sentences that are really next to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a1246b-6414-4dde-9ed8-6270d8a088f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataloader - should be improved\n",
    "# I won't use this dataloader since it is too slow + with the logic in bert_cleaned we are training model only with one sentence\n",
    "# which is incorrect\n",
    "# Will use this only for inference\n",
    "def make_batch():\n",
    "    batch = []\n",
    "    positive = negative = 0  #count of batch size;  we want to have half batch that are positive pairs (i.e., next sentence pairs)\n",
    "    while positive != BATCH_SIZE/2 or negative != BATCH_SIZE/2:\n",
    "        \n",
    "        #randomly choose two sentence so we can put [SEP]\n",
    "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n",
    "        #retrieve the two sentences\n",
    "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "\n",
    "        #1. token embedding - append CLS and SEP\n",
    "        input_ids = [word2id['[CLS]']] + tokens_a + [word2id['[SEP]']] + tokens_b + [word2id['[SEP]']]\n",
    "\n",
    "        #2. segment embedding - [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        #3. mask language modeling\n",
    "        #masked 15%, but should be at least 1 but does not exceed max_mask\n",
    "        n_pred =  min(MAX_MASK, max(1, int(round(len(input_ids) * 0.15))))\n",
    "        #get the pos that excludes CLS and SEP and shuffle them\n",
    "        cand_maked_pos = [i for i, token in enumerate(input_ids) if token != word2id['[CLS]'] and token != word2id['[SEP]']]\n",
    "        shuffle(cand_maked_pos)\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        #simply loop and change the input_ids to [MASK]\n",
    "        for pos in cand_maked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)  #remember the position\n",
    "            masked_tokens.append(input_ids[pos]) #remember the tokens\n",
    "            #80% replace with a [MASK], but 10% will replace with a random token\n",
    "            if random() < 0.1:  # 10%\n",
    "                index = randint(0, vocab_size - 1) # random index in vocabulary\n",
    "                input_ids[pos] = word2id[id2word[index]] # replace\n",
    "            elif random() < 0.8:  # 80%\n",
    "                input_ids[pos] = word2id['[MASK]'] # make mask\n",
    "            else:  #10% do nothing\n",
    "                pass\n",
    "\n",
    "        # pad the input_ids and segment ids until the max len\n",
    "        n_pad = MAX_LEN - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        # pad the masked_tokens and masked_pos to make sure the lenth is max_mask\n",
    "        if MAX_MASK > n_pred:\n",
    "            n_pad = MAX_MASK - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "        #check if first sentence is really comes before the second sentence\n",
    "        #also make sure positive is exactly half the batch size\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < BATCH_SIZE / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < BATCH_SIZE/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
    "            negative += 1\n",
    "            \n",
    "    return batch\n",
    "\n",
    "\n",
    "# Dataset\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, sentences, token_list, word2id, id2word, vocab_size, max_len=128, max_mask=10):\n",
    "        self.sentences = sentences\n",
    "        self.token_list = token_list\n",
    "        self.word2id = word2id\n",
    "        self.id2word = id2word\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.max_mask = max_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Randomly select two sentences (positive or negative pair)\n",
    "        tokens_a_index = random.randint(0, len(self.sentences) - 2)  # Ensure a next sentence exists\n",
    "        is_next = random.random() < 0.5  # 50% chance to be a positive pair\n",
    "\n",
    "        if is_next:\n",
    "            tokens_b_index = tokens_a_index + 1\n",
    "        else:\n",
    "            tokens_b_index = random.randint(0, len(self.sentences) - 1)\n",
    "            while tokens_b_index == tokens_a_index + 1:\n",
    "                tokens_b_index = random.randint(0, len(self.sentences) - 1)\n",
    "\n",
    "        tokens_a = self.token_list[tokens_a_index]\n",
    "        tokens_b = self.token_list[tokens_b_index]\n",
    "\n",
    "\n",
    "         # 1. Token Embedding (Insert CLS and SEP)\n",
    "        input_ids = [self.word2id['[CLS]']] + tokens_a + [self.word2id['[SEP]']] + tokens_b + [self.word2id['[SEP]']]\n",
    "\n",
    "        # 2. Segment Embedding\n",
    "        segment_ids = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        # 3. Masked Language Modeling (15% tokens masked)\n",
    "        valid_pos = [i for i in range(1, len(input_ids) - 1) if input_ids[i] != self.word2id['[SEP]']]\n",
    "        random.shuffle(valid_pos)\n",
    "        n_pred = min(self.max_mask, max(1, int(len(input_ids) * 0.15)))\n",
    "        masked_pos = valid_pos[:n_pred]\n",
    "        masked_tokens = [input_ids[pos] for pos in masked_pos]\n",
    "\n",
    "        for pos in masked_pos:\n",
    "            prob = random.random()\n",
    "            if prob < 0.8:  # 80% chance to replace with [MASK]\n",
    "                input_ids[pos] = self.word2id['[MASK]']\n",
    "            elif prob < 0.9:  # 10% chance to replace with a random token\n",
    "                input_ids[pos] = self.word2id[self.id2word[random.randint(0, self.vocab_size - 1)]]\n",
    "            # 10% chance remains unchanged\n",
    "\n",
    "        # Padding\n",
    "        pad_len = self.max_len - len(input_ids)\n",
    "        input_ids.extend([0] * pad_len)\n",
    "        segment_ids.extend([0] * pad_len)\n",
    "\n",
    "        mask_pad_len = self.max_mask - n_pred\n",
    "        masked_tokens.extend([0] * mask_pad_len)\n",
    "        masked_pos.extend([0] * mask_pad_len)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"segment_ids\": torch.tensor(segment_ids, dtype=torch.long),\n",
    "            \"masked_tokens\": torch.tensor(masked_tokens, dtype=torch.long),\n",
    "            \"masked_pos\": torch.tensor(masked_pos, dtype=torch.long),\n",
    "            \"is_next\": torch.tensor(1 if is_next else 0, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9731bb46-f302-468c-aa26-b2856e4846b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BERTDataset(sentences, token_list, word2id, id2word, vocab_size, max_len=MAX_LEN, max_mask=MAX_MASK)\n",
    "\n",
    "# Create DataLoader with multi-threading for efficiency\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59e2ff4-29fd-4c22-8a0c-e07b0d9ab42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "for key, value in sample_batch.items():\n",
    "    print(f\"{key}: {value.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b5188-a3cf-47b8-bc24-a8ea2b977b39",
   "metadata": {},
   "source": [
    "## 3. Model\n",
    "\n",
    "Recall that BERT only uses the encoder.\n",
    "\n",
    "BERT has the following components:\n",
    "\n",
    "- Embedding layers\n",
    "- Attention Mask\n",
    "- Encoder layer\n",
    "- Multi-head attention\n",
    "- Scaled dot product attention\n",
    "- Position-wise feed-forward network\n",
    "- BERT (assembling all the components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c258fc-0568-4b08-ac19-01ae0abee488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding\n",
    "# Here we simply generate the positional embedding, and sum the token embedding, positional embedding, and segment embedding together.\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, n_segments, d_model, device):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)      # position embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        #x, seg: (bs, len)\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long).to(self.device)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # (len,) -> (bs, len)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)\n",
    "\n",
    "# attention mask\n",
    "def get_attn_pad_mask(seq_q, seq_k, device):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1).to(device)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n",
    "\n",
    "# encoder\n",
    "# The encoder has two main components:\n",
    "# Multi-head Attention\n",
    "# Position-wise feed-forward network\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_ff, d_k, device):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(n_heads, d_model, d_k, device)\n",
    "        self.pos_ffn       = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
    "        return enc_outputs, attn\n",
    "\n",
    "# Hm, I think its not efficient - rewrite?\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k, device):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([d_k])).to(device)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / self.scale # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_k, device):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_k\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, self.d_v * n_heads)\n",
    "        self.device = device\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attn = ScaledDotProductAttention(self.d_k, self.device)(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = nn.Linear(self.n_heads * self.d_v, self.d_model, device=self.device)(context)\n",
    "        return nn.LayerNorm(self.d_model, device=self.device)(output + residual), attn # output: [batch_size x len_q x d_model]\n",
    "\n",
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
    "        return self.fc2(F.gelu(self.fc1(x)))\n",
    "\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, n_layers, n_heads, d_model, d_ff, d_k, n_segments, vocab_size, max_len, device):\n",
    "        super(BERT, self).__init__()\n",
    "        self.params = {'n_layers': n_layers, 'n_heads': n_heads, 'd_model': d_model,\n",
    "                       'd_ff': d_ff, 'd_k': d_k, 'n_segments': n_segments,\n",
    "                       'vocab_size': vocab_size, 'max_len': max_len}\n",
    "        self.embedding = Embedding(vocab_size, max_len, n_segments, d_model, device)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(n_heads, d_model, d_ff, d_k, device) for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.activ = nn.Tanh()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        # decoder is shared with embedding layer\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, self.device)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
    "        \n",
    "        # 1. predict next sentence\n",
    "        # it will be decided by first token(CLS)\n",
    "        h_pooled   = self.activ(self.fc(output[:, 0])) # [batch_size, d_model]\n",
    "        logits_nsp = self.classifier(h_pooled) # [batch_size, 2]\n",
    "\n",
    "        # 2. predict the masked token\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
    "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
    "        h_masked  = self.norm(F.gelu(self.linear(h_masked)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
    "\n",
    "        return logits_lm, logits_nsp\n",
    "    \n",
    "    def get_last_hidden_state(self, input_ids, segment_ids):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, self.device)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a753fe8-b844-4020-baa8-4abfd997a987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model instantiation\n",
    "model = BERT(\n",
    "    n_layers, \n",
    "    n_heads, \n",
    "    d_model, \n",
    "    d_ff, \n",
    "    d_k, \n",
    "    n_segments, \n",
    "    vocab_size, \n",
    "    MAX_LEN, \n",
    "    device # DataParallel requires main model to be in cuda:0\n",
    ")\n",
    "# Making split into several available gpus\n",
    "# device_ids = [0, 1, 2]  # Maps to CUDA_VISIBLE_DEVICES=[1,2,3], so these are actually GPUs 1,2,3\n",
    "if num_gpus > 1:\n",
    "    model = nn.DataParallel(model) # , device_ids=device_ids)  # Enable multi-GPU training\n",
    "# Moving model to main gpu\n",
    "model.to(device) # DataParallel requires main model to be in cuda:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737c99f2-3c20-4f16-b8dc-2064a71ef6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Take data\n",
    "# batch = make_batch()\n",
    "# # Move into pytorch\n",
    "# input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "# # Move inputs to GPU\n",
    "# input_ids = input_ids.to(device)\n",
    "# segment_ids = segment_ids.to(device)\n",
    "# masked_tokens = masked_tokens.to(device)\n",
    "# masked_pos = masked_pos.to(device)\n",
    "# isNext = isNext.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09ce87a-710a-4d91-b6e0-48b2451c350a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To show the training graph\n",
    "loss_history = []\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "epoch_number = 0\n",
    "\n",
    "# TODO: Move to train function\n",
    "# Wrap the epoch loop with tqdm\n",
    "start_time = time.time()\n",
    "model.train()\n",
    "for epoch in tqdm(range(num_epoch), desc=\"Training Epochs\"):\n",
    "    start_epoch = time.time()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        segment_ids = batch[\"segment_ids\"].to(device)\n",
    "        masked_tokens = batch[\"masked_tokens\"].to(device)\n",
    "        masked_pos = batch[\"masked_pos\"].to(device)\n",
    "        isNext = batch[\"is_next\"].to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)    \n",
    "        #logits_lm: (bs, max_mask, vocab_size) ==> (6, 5, 34)\n",
    "        #logits_nsp: (bs, yes/no) ==> (6, 2)\n",
    "\n",
    "        #1. mlm loss\n",
    "        #logits_lm.transpose: (bs, vocab_size, max_mask) vs. masked_tokens: (bs, max_mask)\n",
    "        loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
    "        loss_lm = (loss_lm.float()).mean()\n",
    "        \n",
    "        #2. nsp loss\n",
    "        #logits_nsp: (bs, 2) vs. isNext: (bs, )\n",
    "        loss_nsp = criterion(logits_nsp, isNext) # for sentence classification\n",
    "        \n",
    "        #3. combine loss\n",
    "        loss = loss_lm + loss_nsp\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging\n",
    "        cur_loss = loss.item()\n",
    "        epoch_loss += cur_loss\n",
    "\n",
    "    avg_loss = epoch_loss / BATCH_SIZE\n",
    "    loss_history.append(avg_loss)\n",
    "    \n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        epoch_number = epoch\n",
    "        best_model = copy.deepcopy(model.state_dict())\n",
    "        # print('Epoch:', '%02d' % (epoch), 'best loss saved =', '{:.6f}'.format(cur_loss))\n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch:', '%02d' % (epoch), 'loss =', '{:.6f}'.format(cur_loss))\n",
    "        elapsed_epoch = time.time() - start_epoch\n",
    "        print(\"Epoch time taken: \", elapsed_epoch)\n",
    "        \n",
    "time_elapsed = time.time() - start_time\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "# Save the model after training\n",
    "model_name = f'bert_model_epoch_{str(epoch_number)}_loss_{str(best_loss)}.pth'\n",
    "torch.save(model.state_dict(), model_name)\n",
    "print(\"Model saved to bert_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebdffad-c09c-4e1f-8857-20a6972f0c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(loss_acc_history, fname=\"train_loss_updated\"):\n",
    "    plt.plot(loss_acc_history, label = 'Validation')\n",
    "    plt.title('Loss per epoch')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.plot(v?al_acc_history, label = 'Validation')\n",
    "    # plt.title('Accuracy per epoch')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "    plt.savefig(f'{fname}.png')\n",
    "\n",
    "plot_data(loss_history, f'train_loss_updated_{time.ctime()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53343c9d-452c-4030-ab73-fc5849a53b79",
   "metadata": {},
   "source": [
    "## 5. Inference\n",
    "\n",
    "Since our dataset is very small, it won't work very well, but just for the sake of demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d9d23f-829b-4e32-9cb8-68e11c31e79f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict mask tokens ans isNext\n",
    "batch = make_batch()\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[2]))\n",
    "print([id2word[w.item()] for w in input_ids[0] if id2word[w.item()] != '[PAD]'])\n",
    "input_ids = input_ids.to(device)\n",
    "segment_ids = segment_ids.to(device)\n",
    "masked_tokens = masked_tokens.to(device)\n",
    "masked_pos = masked_pos.to(device)\n",
    "isNext = isNext.to(device)\n",
    "\n",
    "logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)\n",
    "#logits_lm:  (1, max_mask, vocab_size) ==> (1, 5, 34)\n",
    "#logits_nsp: (1, yes/no) ==> (1, 2)\n",
    "\n",
    "#predict masked tokens\n",
    "#max the probability along the vocab dim (2), [1] is the indices of the max, and [0] is the first value\n",
    "logits_lm = logits_lm.data.cpu().max(2)[1][0].data.numpy() \n",
    "#note that zero is padding we add to the masked_tokens\n",
    "print('masked tokens (words) : ',[id2word[pos.item()] for pos in masked_tokens[0]])\n",
    "print('masked tokens list : ',[pos.item() for pos in masked_tokens[0]])\n",
    "print('masked tokens (words) : ',[id2word[pos.item()] for pos in logits_lm])\n",
    "print('predict masked tokens list : ', [pos for pos in logits_lm])\n",
    "\n",
    "#predict nsp\n",
    "logits_nsp = logits_nsp.cpu().data.max(1)[1][0].data.numpy()\n",
    "print(logits_nsp)\n",
    "print('isNext : ', True if isNext else False)\n",
    "print('predict isNext : ',True if logits_nsp else False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
