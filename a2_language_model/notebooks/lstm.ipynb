{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25e69c03-8635-4a57-9bae-5b6f49b4fadc",
   "metadata": {},
   "source": [
    "# Task 1 Dataset Acquisition\n",
    "# Dataset Description: Harry Potter LSTM Dataset\n",
    "\n",
    "**Dataset Name:** Harry Potter LSTM Dataset  \n",
    "**Source:** [Hugging Face Datasets](https://huggingface.co/datasets/KaungHtetCho/Harry_Potter_LSTM)  \n",
    "**Creator:** Kaung Htet Cho  \n",
    "\n",
    "## Description\n",
    "The Harry Potter LSTM dataset is a text dataset derived from the Harry Potter book series. It consists of unstructured text data prepared for natural language processing (NLP) tasks, specifically language modeling. This dataset is designed for sequence-to-sequence learning and is commonly used to train models such as LSTMs and Transformers to generate text in the style of the Harry Potter universe.\n",
    "\n",
    "## Purpose\n",
    "The dataset is suitable for various NLP tasks, including:\n",
    "- Text generation\n",
    "- Language modeling\n",
    "- Sequence-to-sequence learning\n",
    "\n",
    "## Source Attribution\n",
    "The dataset is hosted on the Hugging Face Datasets platform and was created by Kaung Htet Cho. Proper credit has been provided to the dataset's source and creator.\n",
    "\n",
    "## License\n",
    "Refer to the [Hugging Face page](https://huggingface.co/datasets/KaungHtetCho/Harry_Potter_LSTM) for licensing details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b14a0fd5-b8d3-4749-9963-c4f079a368dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext, datasets, math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c60fdd89-e097-40e1-ba2f-aa8ceddfc296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d51b7066-70c8-4475-890a-197a9050f7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a734ea60f81446ab994d48417da0ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833fe8e18b154f40ab0d39942c5f46aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)in/Harry Potter 1 - Sorcerer's Stone.txt:   0%|          | 0.00/443k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be29580967f41aca6a739b538abdd43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)/Harry Potter 2 - Chamber of Secrets.txt:   0%|          | 0.00/490k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f389745b504e848b6073e53229073d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)y Potter 3 - The Prisoner Of Azkaban.txt:   0%|          | 0.00/615k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb7868c876f34f50aca617c687c5d0c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)/Harry Potter 4 - The Goblet Of Fire.txt:   0%|          | 0.00/1.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bccdad50fc24a61ab00626182a65602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)arry Potter 5 - Order of the Phoenix.txt:   0%|          | 0.00/1.49M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2a29318ebf4df8aa9a9c8b6e666244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)rry Potter 6 - The Half Blood Prince.txt:   0%|          | 0.00/986k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7f8b87c4f74df7b34ee9560331580e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)est/Harry Potter 7 - Deathly Hollows.txt:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44d796f06f5497886d9f86d471c7359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/57435 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c96964afd164bb983eff01e425ae6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/5897 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603e581f15ab4631b7ec8b7950305378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/6589 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 57435\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 5897\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 6589\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "# dataset = datasets.load_dataset('microsoft/LCC_python') # not enough gpu memory\n",
    "# dataset = datasets.load_dataset('codeparrot/github-jupyter-text-code-pairs') # not enough gpu\n",
    "# dataset = datasets.load_dataset('codeparrot/github-jupyter-code-to-text') # not enough gpu\n",
    "# will proceed with harry potter\n",
    "\n",
    "dataset = datasets.load_dataset('KaungHtetCho/Harry_Potter_LSTM')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db162284-0f42-4721-a437-c9333809b92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'None of them noticed a large, tawny owl flutter past the window. '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][14]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237cab69-e90f-41e2-91dd-2011c524e0fa",
   "metadata": {},
   "source": [
    "# Task 2 Model Training\n",
    "\n",
    "### Steps for Preprocessing Text Data\n",
    "\n",
    "1. **Dataset Loading**  \n",
    "   The dataset was loaded using the Hugging Face `datasets` library to ensure ease of access and compatibility with NLP frameworks.\n",
    "\n",
    "2. **Text Tokenization**  \n",
    "   A `basic_english` tokenizer was used to split text into lowercased word tokens, removing punctuation and unnecessary formatting.\n",
    "\n",
    "3. **Removing Empty Text Entries**  \n",
    "   Entries with empty or whitespace-only `text` fields were filtered out to retain only meaningful text data.\n",
    "\n",
    "4. **Tokenizing the Dataset**  \n",
    "   Each text entry was tokenized to create a list of tokens, preparing the dataset for language modeling tasks.\n",
    "\n",
    "5. **Removing Empty Token Lists**  \n",
    "   Any entries that resulted in empty token lists after tokenization were filtered out to ensure a clean dataset.\n",
    "\n",
    "6. **Building the Vocabulary**  \n",
    "   A vocabulary was built from the tokenized dataset with a minimum frequency threshold. Special tokens such as `<unk>` and `<eos>` were added to handle unknown words and mark the end of sentences.\n",
    "\n",
    "7. **Setting Default Index for Unknown Tokens**  \n",
    "   The `<unk>` token was set as the default index to handle out-of-vocabulary words during model training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5bb35ae-9fdc-4d77-b6ce-58e9bc80b96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'boa',\n",
       " 'constrictor',\n",
       " 'jabbed',\n",
       " 'its',\n",
       " 'tail',\n",
       " 'at',\n",
       " 'the',\n",
       " 'sign',\n",
       " 'again',\n",
       " 'and',\n",
       " 'harry',\n",
       " 'read',\n",
       " 'on',\n",
       " 'this',\n",
       " 'specimen',\n",
       " 'was',\n",
       " 'bred',\n",
       " 'in',\n",
       " 'the',\n",
       " 'zoo',\n",
       " '.',\n",
       " 'oh',\n",
       " ',',\n",
       " 'i',\n",
       " 'see',\n",
       " '--',\n",
       " 'so',\n",
       " 'you',\n",
       " \"'\",\n",
       " 've',\n",
       " 'never',\n",
       " 'been',\n",
       " 'to',\n",
       " 'brazil',\n",
       " '?']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "filtered_dataset = dataset.filter(lambda example: example['text'].strip() != '')\n",
    "\n",
    "tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['text'])}\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_data, remove_columns=[], fn_kwargs={'tokenizer': tokenizer})\n",
    "filtered_tokenized_dataset = tokenized_dataset.filter(lambda example: len(example['tokens']) > 0)\n",
    "filtered_tokenized_dataset['train'][212]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52796e50-bcb4-4030-9b92-559f873c755f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9803, ['<unk>', '<eos>', '.', ',', 'the', 'and', 'to', \"'\", 'of', 'a'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    tokenized_dataset['train']['tokens'], \n",
    "    min_freq=3,\n",
    "    specials=['<unk>', '<eos>'],\n",
    "    special_first=True\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "len(vocab), vocab.get_itos()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dffa2fdf-3724-4459-9319-aef9629975b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []\n",
    "    for example in dataset:\n",
    "        if example['tokens']:\n",
    "            tokens = example['tokens'].append(\"<eos>\")\n",
    "            tokens = [vocab[token] for token in example['tokens']]\n",
    "            data.extend(tokens)\n",
    "    data = torch.LongTensor(data)\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    data = data[:num_batches * batch_size]\n",
    "    data = data.view(batch_size, num_batches)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a01f652b-99ef-4f90-9dd9-4580a144e910",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
    "valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
    "test_data  = get_data(tokenized_dataset['test'], vocab, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486e048-eb92-44a6-837d-2589aa45a085",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "The model used for training is an **LSTM-based Language Model** designed to predict the next word in a sequence. Below are the key components of the architecture:\n",
    "\n",
    "1. **Embedding Layer**  \n",
    "   - Converts input tokens (word indices) into dense vector representations of a fixed size (`emb_dim`).\n",
    "   - Initialized uniformly within a small range to ensure stable training.\n",
    "\n",
    "2. **LSTM Layers**  \n",
    "   - A multi-layer Long Short-Term Memory (LSTM) network with `num_layers` layers and a hidden dimension of `hid_dim`.\n",
    "   - Incorporates dropout regularization (`dropout_rate`) to reduce overfitting.\n",
    "   - Processes the sequential input and learns temporal dependencies in the data.\n",
    "\n",
    "3. **Dropout Layer**  \n",
    "   - Adds dropout after the embedding and LSTM layers to further regularize the model and improve generalization.\n",
    "\n",
    "4. **Fully Connected Layer**  \n",
    "   - Maps the output of the LSTM to the vocabulary size (`vocab_size`) to produce logits for each token in the vocabulary.\n",
    "   - The weights and biases are initialized with uniform distribution for better convergence.\n",
    "\n",
    "5. **Initialization**  \n",
    "   - Custom initialization of weights for the embedding, fully connected, and LSTM layers to ensure stable and efficient training.\n",
    "\n",
    "6. **Hidden State Management**  \n",
    "   - The `init_hidden` method initializes the LSTM's hidden and cell states with zeros.\n",
    "   - The `detach_hidden` method detaches hidden states from the computation graph during training to prevent backpropagation through time from exceeding the current batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7281aa08-fe31-498b-8396-68310d16fe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "        super(LSTMLanguageModel, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hid_dim    = hid_dim\n",
    "        self.emb_dim    = emb_dim\n",
    "\n",
    "        self.embedding  = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm       = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout    = nn.Dropout(dropout_rate)\n",
    "        self.fc         = nn.Linear(hid_dim, vocab_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_range_emb   = 0.1\n",
    "        init_range_other = 1 / math.sqrt(self.hid_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim, self.hid_dim).uniform_(-init_range_other, init_range_other)\n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.emb_dim, self.hid_dim).uniform_(-init_range_other, init_range_other)\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "    def detach_hidden(self, cells):\n",
    "        hidden, cell = cells\n",
    "        hidden = hidden.detach()\n",
    "        cell = cell.detach()\n",
    "\n",
    "        return hidden, cell\n",
    "\n",
    "    def forward(self, src, hidden):\n",
    "        embedding = self.dropout(self.embedding(src)) # [batch_size, seq_len]\n",
    "        output, hidden = self.lstm(embedding, hidden) # [batch_size, seq_len, hid_dim]\n",
    "\n",
    "        output     = self.dropout(output)\n",
    "        prediction = self.fc(output)\n",
    "\n",
    "        return prediction, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ad24e2f-affc-4580-9824-66aee1e9f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size   = len(vocab)\n",
    "emb_dim      = 1024  # 400 in the paper\n",
    "hid_dim      = 1024  # 1150 in the paper\n",
    "num_layers   = 2     # 3 in the paper\n",
    "dropout_rate = 0.65\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d42e01e0-7b1c-46d2-9770-c68f87e7320d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 36,879,947 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "vocab_size   = len(vocab)\n",
    "emb_dim      = 1024  # 400 in the paper\n",
    "hid_dim      = 1024  # 1150 in the paper\n",
    "num_layers   = 2     # 3 in the paper\n",
    "dropout_rate = 0.65\n",
    "lr = 1e-3\n",
    "\n",
    "model = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "61b35a48-c7a4-4fc3-aed8-58a704e298b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    src    = data[:, idx:idx+seq_len]\n",
    "    target = data[:, idx+1:idx+seq_len+1]\n",
    "    return src, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1064dc2-9311-48d5-ac6e-39e4e21fe826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    num_batches = data.shape[-1]\n",
    "    data        = data[:, :num_batches - (num_batches - 1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training', leave=False):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, idx)\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size  = src.shape[0]\n",
    "        \n",
    "        prediction, hidden = model(src, hidden)\n",
    "\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "        target     = target.reshape(-1)\n",
    "        loss       = criterion(prediction, target)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "\n",
    "    return epoch_loss / num_batches\n",
    "\n",
    "\n",
    "def valid(model, data, criterion, batch_size, seq_len, device):\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    \n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches - 1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size = src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "\n",
    "    return epoch_loss / num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "475b8eb2-fb72-4e21-b942-4263e4f38477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m epoch_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, n_epochs):\n\u001b[0;32m---> 14\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m valid(model, valid_data, criterion, batch_size, seq_len, device)\n\u001b[1;32m     17\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep(valid_loss)\n",
      "Cell \u001b[0;32mIn[44], line 30\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data, optimizer, criterion, batch_size, seq_len, clip, device)\u001b[0m\n\u001b[1;32m     27\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip)\n\u001b[1;32m     28\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 30\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m seq_len\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m epoch_loss \u001b[38;5;241m/\u001b[39m num_batches\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "seq_len  = 50 # decoding length\n",
    "clip     = 0.25\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "import time\n",
    "\n",
    "epoch_start = time.time()\n",
    "\n",
    "for epoch in range(0, n_epochs):\n",
    "    train_loss = train(model, train_data, optimizer, criterion, batch_size, seq_len, clip, device)\n",
    "    valid_loss = valid(model, valid_data, criterion, batch_size, seq_len, device)\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), f\"models/best_model_lstm_lm.pt\")\n",
    "\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')\n",
    "\n",
    "    print(f'\\t[{str(epoch + 1)}/{str(n_epochs)}] epochs progress')\n",
    "\n",
    "elapsed_epoch = time.time() - epoch_start\n",
    "print(\"Train time taken: \", elapsed_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dc001837-3f0f-4636-bddf-02baf8e08f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 83.111\n"
     ]
    }
   ],
   "source": [
    "vocab_size   = len(vocab)\n",
    "emb_dim      = 1024  # 400 in the paper\n",
    "hid_dim      = 1024  # 1150 in the paper\n",
    "num_layers   = 2     # 3 in the paper\n",
    "dropout_rate = 0.65\n",
    "lr = 1e-3\n",
    "\n",
    "model = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "model.load_state_dict(torch.load('models/best_model_lstm_lm.pt',  map_location=device))\n",
    "\n",
    "test_loss = valid(model, test_data, criterion, batch_size, seq_len, device)\n",
    "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "473deb54-93d7-4011-b924-34cdcde8b7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            \n",
    "            #prediction: [batch size, seq len, vocab size]\n",
    "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
    "            \n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
    "                break\n",
    "\n",
    "            indices.append(prediction) #autoregressive, thus output becomes input\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7f5d5d88-c8e2-487f-9858-f3269c769d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n",
      "harry potter is a bit of a\n",
      "\n",
      "0.3\n",
      "harry potter is a bit of a joke .\n",
      "\n",
      "0.5\n",
      "harry potter is being mistreated ,\n",
      "\n",
      "0.7\n",
      "harry potter is being mistreated , sir . that was well a prefect .\n",
      "\n",
      "0.75\n",
      "harry potter is being mistreated , sir . that was well a prefect , and you would have been cleared with him . . .\n",
      "\n",
      "0.8\n",
      "harry potter is being mistreated , sir . that was well a prefect , and you would have been cleared with him . . .\n",
      "\n",
      "1.0\n",
      "harry potter is being mistreated , sir . that was well worked . . . .\n",
      "\n",
      "1.8\n",
      "harry potter is being sure you gave the new committee ! well should went devils , hurry down it . . give stew back pig ? dead me stupid , pay gifts kill malfoy and twice gurg still . ­ his unpleasantly grunt . as tree issued he placed me nobody never knew viktor been petrified ­ hissed them in fact in gold . now rolled down closer for harrys farewell black over weeks , both acted above voldemort and mostafa helped to gryffindors forward fair .\n",
      "\n",
      "1.9\n",
      "harry potter is being sure isn ' tell madam fat opportunity well worked . ive stumped hurry down it dodge doing mad-eye stew back up ? seemed me caught binky pay safely in malfoy going time gurg weak . makes his end while those as tree issued he have been nobody never knew sorry . harry put himself together in pieces in gold . there rolled connected closer for lifted between riffraff jars like interest since nicolas answering decided and mostafa helped was gryffindors forward fair .\n",
      "\n",
      "2.1\n",
      "harry potter is being sure isn ' tell madam fat opportunity well worked . ive stumped hurry onto it dodge doing mad-eye stew back pig ? seemed me caught feelings pay safely in malfoy going time gurg weak . makes his end while those as tree issued he placed me nobody never knew viktor been petrified ­ hissed them in eighteen crates , and now mcgonagall puts closer for harrys farewell hannah disguise once , both acted above voldemort and mostafa helped to gryffindors forward fair .\n",
      "\n",
      "3.1415\n",
      "harry potter is being stumbled piercing tale snapping pouring drawn november well pipes went prefect indignantly hurry\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Harry Potter is '\n",
    "max_seq_len = 100\n",
    "seed = 0\n",
    "\n",
    "#smaller the temperature, more diverse tokens but comes \n",
    "#with a tradeoff of less-make-sense sentence\n",
    "temperatures = [0.25, 0.3, 0.5, 0.7, 0.75, 0.8, 1.0, 1.8, 1.9, 2.1, 3.1415]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
